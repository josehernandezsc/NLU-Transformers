# NLU-Transformers
Implementation of self-attention mechanism (Vaswani et al.) to develop a Transformer for NMT. Here, Europarl dataset was used. I compared both a LSTM seq2seq encoder-decoder model with an attention mechanism and implemented a lexical mechanism as well. This was compared with the Transformer to analyze performance on different data volumes and settings. One major finding was the higher performance achieved by the Transformer only in higher data volume settings, compared to the LSTM. Nevertheless, the lexical mechanism on the LSTM had a better performance when using fewer data than the Transformer.
